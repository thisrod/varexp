\nonstopmode
\input a4
\input xpmath
\input respnotes \tenrm

\title{Stability of variational quantum dynamics with Gaussian approximants}

\def\G{|Γ〉}
\def\Gx{|Γ_χ〉}
\def\cite#1{$[\hbox{\tt #1}]$}

%% Background

When they realised that positions and momenta are essentially uncertain, the first quantum physicists were bound to discover the gaussian wave packet by seeking conditions that produce a normal distribution of probability.  They soon understood the free particle and turned to the harmonic oscillator, investigating the form of its ground state and the way its excited states generate sinusoidal motion in the classical limit.  These problems were also solved by the gaussian wave packet.  With its width fixed by equipartition of energy and the uncertainty principle, it became the coherent state.

The simplest mechanical systems allow Schrödinger's law of motion to be solved in closed form, but most quantum problems must be solved approximately.  This can be done variationally, specifying a class of approximants that can be manipulated exactly, then finding one of them that most nearly solves the problem.  To test how well a wave function approximates the dynamics of a system, the difference between the two sides of Schrödinger's equation can be measured by the Hilbert space norm.  It remains to choose a class of approximants.  The gaussian wave packets are an common choice, because they differ in their scale and their central position and momentum, useful properties to determine for any quantum state.

The invention of the laser called for a quantum theory of intense radiation.  Laser beams are hard to analyse because their quantum descriptions are so large, comprising up to $10^{18}$ photons distributed across thousands of modes.  The theory was made managable by Glauber and Sudarshan, who showed that any quantum state can be expanded as a superposition of coherent states.  Laser light is very coherent, so these expansions often represent it efficiently.  Moyal(?) developed an efficient way to calculate dynamics, leading to a family of phase space Monte-Carlo methods.  When very coherent matter, in the form of atomic Bose-Einstein condensates, became easy to access in the lab, phase space methods were sucessfully applied to calculate its dynamics, and have since been applied to other forms of condensed matter.  They have solved many problems, but are limited by intractable convergence issues.

In the 1970s, Heller put the two ideas together, proposing to simulate quantum dynamics variationally, using finite superpositions of coherent states as the approximants.  This is an appealing approach.  Every quantum state can be approximated arbitrarily well as the number of components in the superposition increases.  The experience of quantum optics shows that, in many systems, the approximation converges with a reasonable number of components.  Also, variational dynamics should avoid the covergence problems that limit the Monte-Carlo methods.

%% Problem

Since Heller suggested it, there have been several attempts to apply this numerically, but none has entirely succeeded.  In all cases, numerical instability has prevented the superposition being expanded to the point of convergence.
%% Consequence
These attempts have aimed to solve chemistry problems, involving dozens of atoms, where the lack of convergence limited the precision that could be obtained, but superpositions with a few components, and ad-hoc methods to improve the stability, still gave useful results.  The kind of problems that people approach with phase space methods come from atom optics or condensed matter; they involve thousands or millions of atoms, and the convergence problems would prevent any useful information being obtained.  A systematic approach to regularisation is required in order to approach these problems variationally.  The prospective rewards are great: these problems include high temperature superconductivity and the dynamics of the early universe, topics on which today's physicists expend massive efforts.

%% Overview

There appear to be two sorts of instabilities.  The first, static, sort occurs when a quantum state is expanded over a finite set of coherent states, and those states have irregular amplitudes.  This is similar to the kind that numerical analysts call ill-posed problems, and the methods they use to analyse and resolve them can be applied here.  However, the cause of ill-posedness in our problem differs from those usually studied, and the usual methods of resolution don't work as well as they usually do.  There is one advantage of the expansion problem: because it is iterated at each timestep, the dynamics of the amplitudes can be exploited to so that they posed better expansion problems as the simulation progresses.

The second type of instability is a dynamical one.  This occurs whenever Schrödinger's equation is integrated with a quartic hamiltonian, over discrete timesteps.  In the second quantised picture, it occurs because the repulsion between particles causes components where many particles occupy the mode to oscillate very rapidly.  The simulation becomes stable when these components are removed, but that changes the dynamics.  Integrating Schrödinger's equation with repulsive particles either requires very short timesteps, or special integration formulae that exploit features of Hamiltonian dynamics.  Applying these formulae to the variational dynamics of the coherent states, which are not Hamiltonian, is somewhat subtle.

\beginsection{The phase space expansion operator}

Expansion over coherent states is a linear operator from functions of a complex variable to quantum states,
$$\G f=π^{-½}\int |α〉e^{-½|α|²}f(α*)\,d²α.$$
The notation $\int f(α)\,d²α$ means $\int_{-∞}^{∞}\int_{-∞}^{∞} f(x+iy)\,dx\,dy$.  The function $f$ is such that $e^{-½|α|²}f(α)$ is square-integrable.  Bargmann\cite{pam-14-187} showed that these functions form a Hilbert space under the inner product $〈g,f〉=\int e^{-|α|²}g*(α)f(α)\,d²α$; this paper refers to it as the space of phase space distributions, or just distributions.  When norms of functions are written below, the norm is the one derived from this inner product.  In this paper, $\G f$ denotes the ket that results from applying the linear operator $\G$ to the phase space distribution $f$; application and composition of linear operators is shown by juxtaposition, and functions whose values are quantum states are written as kets, usually with a greek letter.

The state $|α〉$ is a coherent state with complex amplitude $α$.  These can be defined in a number of ways, perhaps most simply in second quantisation, as eigenstates of a boson annihilation operator $a$, with $a|α〉=α|α〉$.  The annihilation operator is not hermitian, so the eigenvalues $α$ are complex numbers.  In first quantisation, such an operator is the complex amplitude
$$a=(2\hbar)^{-½}(λx+ip/λ),$$
where $x$ and $p$ are the position and momentum operators for a particle with mass $m$.  The bosons in this case are vibrational quanta in a notional harmonic potential, centred at the origin, with frequency $ω=λ²/m$.

Coherent states are not orthogonal, but their brackets follow the law
$$〈β|α〉=e^{-½|β|²-½|α|²+β*α},$$
whence
$$\bigl|〈β|α〉\bigr|²=e^{-|β-α|²}.$$
When kets are considered as vectors, the angle between coherent states satisfies
$$\cos θ=‖|α〉‖‖|β〉‖\cos θ=|〈β|α〉|=e^{-½|β-α|²},$$
and comparing Taylor series gives
$$θ=|β-α|+O(|β-α|²).$$
This has a simple geometric analogy where phase space is mapped locally onto the surface of the unit sphere, the coherent states are vectors from the origin to the surface, and $|α-β|$ is a spherical angle.  The analogy fails when the angles between vectors are no longer small; for example, it is misleading in regard to the relation between $|α〉$ and $∂_α|α〉$.

The projector satisfies
$$\int |α〉〈α|\,d²α=π.$$
This is similar to the law for an orthonormal basis, except that the right hand side is $π$ instead of 1.  Further properties of the coherent states will be stated as required.  Glauber's summary of them\cite{prx-131-2766} is paraphrased in every quantum optics textbook ever written.

Bargmann showed that the Hilbert space of wavefunctions is isomorphic to the space of phase space distributions, under the isometry $〈Γ⁺|$, where
$$〈 Γ⁺|ψ〉(α)=π^{-½}e^{½|α|²}〈 α|ψ〉.$$
Being a linear operator from quantum states to phase space functions, $〈Γ⁺|$ is written as a ket.  It is a pseudoinverse of $|Γ〉$, in the sense that $|Γ〉〈 Γ⁺|$ is the identity operator in state space.  The other composition, $〈 Γ⁺|Γ〉$, projects the functions of a complex variable onto the entire functions, orthogonally under the Bargmann inner product.  (Prove that, or find Bargmann's proof.)  It follows that $f=〈 Γ⁺|ψ〉$ is the function with least norm that satisfies $|Γ〉 f=|ψ〉$, with $‖f‖=\bigl‖ |ψ〉\bigr‖$.

If, following Bargmann, we identify state space with the entire functions, $|Γ〉$ is identified with the projector $〈 Γ⁺|Γ〉$.  Therefore, although $|Γ〉$ is singular, the singularity is easily resolved.  If we constrain the norm of $f$ to be not much greater than the norm of $|ψ〉$, which can be done simply enough by Tychonov regularisation, we ensure that $f$ is close to $〈 Γ⁺|ψ〉$.  And $〈 Γ⁺|$, an isometry, is as nice a function as could possibly be desired.  So it is curious that the discrete version of this problem has caused so much trouble.

%% Consequences of Γ being a projection

The part of $|Γ〉$ that projects functions onto entire functions can be written as follows
$$π^{-½}〈β|Γ〉f=π^{-1}\int 〈β|α〉e^{-½|α|²}f(α*)\,d²α=π^{-1}\int 〈0|D(α-β)|0〉e^{-½|α|²}f(α*)\,d²α.$$
This appears to be a convolution of the function $e^{-½|α|²}f(α*)$ with the kernel $〈0|D(α)|0〉$.  There are many theorems to the effect that real convolution operators are badly conditioned.  It is interesting that these results do not apply to the complex convolution $|Γ〉$: as a projector, it is as nice an operator as it could possibly be.  I think the loophole is that of real functions can only oscillate at a limited rate, so that their Fourier transforms have finite norm.  Complex functions have space in the Argand plane to oscillate arbitrarily rapidy.  And the kernel $〈0|D(α)|0〉$ does: it is a wave packet with carrier frequency proportional to the imaginary part of $α$.

\beginsection{Truncated phase space}

The operator $|Γ〉$ acts on functions defined over the whole complex plane.  For states with high particle number, the smallest preimages have support far from the origin.  We can not compute with such functions: any discretisation will sample values of $f$ on a finite subset of the plane.  Unfortunately, the conditioning of these restricted operators will be worse than that of $|Γ〉$.  We have excluded the small preimages of the states with large particle number, so they must be represented by functions with support close to the origin, but with large norms.  Hence the restricted operators will be badly conditioned.

%% The SVE

The above paragraph can be stated quantitatively.  Define a restricted operator
$$\Gx f=π^{-½}\int |α〉e^{-½|α|²}χ(α*)f(α*)\,d²α.$$
Most obviously, $χ$ could be a characteristic function, with $χ(z)$ being 1 for $|z|≤b$ and 0 outside this disk.  The formalism allows gradual truncation too.

The sizes of states and their expansions can be read from the singular value expansion of $\Gx $.   If the domain and range of $\G$ were the same space, it would have an eigenvalue decomposition; the SVE is a generalisation of that.  It has the form
$$\Gx f = ∑_{i=1}^∞ |u_i〉σ_i〈v_i,f〉.$$
The kets $|u_i〉$ are an orthonormal basis of state space, known as the left singular vectors of $\Gx$.  The $σ_i$ are positive real numbers, called singular values.  The functions $v_i$ are are orthonormal under the Bargmann inner product, and are called right singular vectors.

In the case where $χ(z)=1$, and $\Gx=\G$, the expansion is quite arbitrary.  The left singular kets can be any orthonormal basis, the right singular vector $v_i$ is the unique entire function for which $\G v_i=|u_i〉$, and every singular value is 1.  If the domain and range of $\G$ were the same, this would indicate that it was a projector.  The component of $|ψ〉$ in the null space of $\G$ is discarded, being orthogonal to every $v_i$, while the other component retains its expansion coefficients as the basis $v_i$ is replaced by $|u_i〉$.

When the operator is restricted, the SVE gives more information. If $\Gx$ were one to one, each $v_i$ would be the unique preimage of $|g_i〉$; since $\Gx$ is singular, $v_i$ is the preimage with least norm.  The singular values are the ratios of $‖\Gx v_i‖=‖σ_i|u_i〉‖$ to $‖v_i‖$.  If $|u_i〉$ has a low particle number, it can be expanded efficiently: $v_i$ will be close to $〈Γ⁺|u_i〉$, and $σ_i$ will be close to 1.  If $|u_i〉$ includes components with large particle number, its expansion will have a larger norm than itself.  In this case, $v_i$ will be a truncation of $〈Γ⁺|u_i〉$ to small amplitudes, multiplied by a large constant to maintain normalisation, and $σ_i$ will be small.

Before we study the singular values and vectors of $\Gx$, consider why small singular values are a problem.  The pseudoinverse of $\Gx$ with least norm is
$$〈Γ⁺_χ|=∑_{i=1}^∞ v_iσ_i^{-1}〈u_i|.$$  Suppose this is applied to a state $|ψ〉$ that has a low expected particle number, but small components with large particle number.  Such a component will overlap some $|u_i〉$ belonging to small $σ_i$, and $〈Γ⁺_χ|$ will multiply it by $σ_i^{-1}$, generating large components of those $v_i$ in the expansion function.  In the continuous expansion, this isn't a problem: the information is still there, and the state can be recovered by applying $\Gx$ to the expansion.  When the expansion is discretised, however, the small, low particle number component of $〈Γ⁺_χ|ψ〉$ that contains the important information about $|ψ〉$ is likely to be lost in the large component of $〈Γ⁺_χ|ψ〉$ that gives minor details of high particle number components of $|ψ〉$.

Let's compute some singular values and vectors.  Suppose that $χ$ is the characteristic function of a disk, with
$$χ(α*)=\cases{1 & $|α|≤b$ \cr 0 & otherwise.}$$
From symmetry, let's guess that the singular kets are the number states—we will check this numerically later.  It can be shown by a Lagrange multiplier argument that the right singular functions are the truncations to the disk of $f_n=〈Γ⁺|n〉$.  Then the singular values are
$$σ_n={‖\Gx f_n‖\over ‖f_n‖}.$$
We take
$$f_n=π^{-½}〈α|n〉e^{½|α|²}=π^{-½}∑_{n=0}^∞ {α^{n\ast}\over \sqrt{n!}}.$$

First evaluate $‖f_n‖$, using the Bargmann norm.  We have
$$\eqalign{ ‖f_n‖²&={1\over π}\int_{|α|≤b} e^{-|α|²}{|α|^{2n}\over n!}\,d²α \cr
	&={1\over π}\int_0^b e^{-r²}{r^{2n}\over n!}2πr\,dr \cr
	&={1\over n!}\int_0^{b²} u^ne^{-u}\,du=Γ(n+1,b²),}$$
where $Γ(n,x)$ is the incomplete gamma function, as defined in Matlab.  (Everyone defines this a different way.)  The other norm is given by
$$\eqalign{ \Gx f_n&=π^{-½}\int_{|α|≤b} |α〉e^{-½|α|²}f_n(α*)\,d²α \cr
	&={1\over π}∑_{m=0}^∞\int e^{-|α|²}{α^{m}\over \sqrt{m!}}|m〉{α^{n\ast}\over \sqrt{n!}}\,d²α \cr
	&={1\over π}\int_0^b dr re^{-r²}∑_{m=0}^∞{r^{m+n}\over \sqrt{m!n!}}|m〉\int_0^{2π} dφe^{i(m-n)φ} \cr
	&=\int_0^b dr 2re^{-r²}{r^{2n}\over n!}|n〉 \cr
	&=Γ(n+1,b²)|n〉.}$$

The singular value comes out to 
$$σ_n={‖ \Gx f_n‖\over ‖f_n‖}=\sqrt{Γ(n+1,b²)}.$$

Numerical experiments in the script {\tt csve.m} support this.  The operator $\G$ was discretised on a grid in phase space, with spacing $h=0.3$, bounded by the circle about the origin of radius $b$ for $b=3$, 4 and 5.  Ket space was discretised by taking inner products with the number states $|0〉$ through $|24〉$.  The graph below shows the singular values of the truncated operator $\Gx$.

\topinsert \XeTeXpicfile svgrid.pdf width \hsize \endinsert

The contour plot shows how the analytic expression for $σ_n$ depends on $n$ and the truncation radius $b$.  The value has decayed to 0.7 when $n=b²$.  The rule of thumb is that we get one large singular value for every unit circle of phase space, with area $π$.

\topinsert \XeTeXpicfile svcont.pdf width \hsize \endinsert

The following three figures show the right singular vectors, sampled versions of phase space functions.  The brightness of the plots indicates $|v_n(α)|$, the colors indicate phase.  These look very much like truncated number states.  As the grid expands, the pre-images of higher number states fit in it, and more singular values have converged.

\topinsert \hbox{\XeTeXpicfile rsv3.pdf width 0.5\hsize
\XeTeXpicfile rsv4.pdf width 0.5\hsize}
\XeTeXpicfile rsv5.pdf width 0.5\hsize\hfil\break
The right singular functions $v_i$ for the truncated expansion operator $\Gx$, where phase space is truncated at $|α|=3$, 4 and 5.  These are functions of a complex variable: the brightness of the plot indicates modulus, and the color indicates phase.\endinsert

Reducing the dimension of Fock space so that the problem is underdetermined means that, when the problem is solved by SVD, we only take components along $N+1$ directions in Fock space, and map them to $N+1$ directions in parameter space.  The remaining $2R-N-1$ components of the solution are zero.  As we add extra dimensions to Fock space, we are taking components in more Fock space directions, which include more oscillating components of the wavefunction.  This is like sampling a function on a finer grid.  With these directions, we can form some of the smaller singular vectors of the full problem.


\beginsection{Gabor frames}


\beginsection{Discretely sampled expansions}

On a computer, the expansion in phase space must be discretised.  The obvious way to do so is to replace the integral with a sum, so that
$$|ψ〉=d₁|α₁〉+⋯+d_R|α_R〉=|A〉d,$$
where $|A〉=\pmatrix{|α₁〉&⋯&|α_R〉}$, and $d$ is a column vector of the coefficients.
In this paper, $c$ will usually denote the coefficients of expansion over orthogonal bases, and $d$ will represent coefficients of nonorthogonal expansions.  If the system has only a few modes, so that $α$ is a low-dimensional vector, the set of sampled amplitudes $A=\{α₁, …,α_R\}$ could be a regular grid; this makes many things easier.  When there are many modes, and $α$ is a large vector, the storage required for grids becomes exponentially large, and the sample points will have to be irregular.

This expansion is formally equivalent to the continuous one, in that
$$∑_r d_r|α_r〉=|Γ〉f_A,$$
where
$$f_A(β)=π^½∑_rd_re^{½|α_r|²}δ(β-α_r).$$
I don't know enough about generalised functions to be comfortable working with these forms; in particular, the norm of $f_A$ is a bit mysterious.  If this could be clarified, it might provide some useful insights, for example how the norm of $f_A$ compares to that of $〈Γ⁺|ψ〉$.

To get a sense of what residuals to expect from optimised expansions, the expansion of a state as an entire function, $f=〈Γ⁺|ψ〉$, can be sampled at discrete amplitudes, and the integral $|Γ〉f$ evaluated as a sum.  In the limit of many amplitudes, this should converge to $|Ψ〉$.  I chose the vacuum state as $|Ψ〉$.  I evaluated the integral in two ways, supposed to be extreme cases.

The first way used a grid, restricted to the disk $|α|<2.5$.  The pitch of the grid, $h$, was varied, and $f(α_r)=π^{-½}e^{½|α_r|²}〈α_r|ψ〉$ evaluated at each point.  The integral was discretised in the obvious way, to give approximations
$$|ψ_R〉={1\over πh²} ∑_{r=1}^R |α_r〉〈α_r|ψ〉.$$
A residual $‖|ψ〉-|ψ_R〉‖$ was found for each $R$; these are plotted as the solid line in the figure.  As the components increase, the residual asymptotes to the phase space truncation error $‖|Γ_χ〉〈Γ⁺|ψ〉-|ψ〉‖$.  (Calculate that precisely.)  The gaussian function $f$ is very smooth, so this converges rapidly as $R$ increases.

The second way was a type of stochastic integration.  The samples $α_r$ were drawn randomly, from the normal distribution $|〈α|ψ〉|$.  The integral was approximated by 
$$|ψ'_R〉=∑_{r=1}^R |α_r〉{〈α_r|ψ〉\over |〈α_r|ψ〉|},\qquad |ψ_R〉={|ψ'_R〉\over||ψ'_R〉|}.$$
The phase of $f$ is integrated by sampling, but the modulus is integrated stochastically.  The residuals are plotted as the broken line.  In principle, this will converge exactly as $R$ increases.  However, the convergence is $R^{-½}$, as seen on the graph, and expected from the laws of large numbers.

These two lines set bounds on how well we can reasonably expect to solve the expansion problem on an irregular grid.  A least squares method that does no better than sampling the argument of $f(α)$ at random $α$ is worthless: we need to do better than the dashed line.  On the other hand, it seems unreasonable to expect samples on an irregular grid to fit $|Ψ〉$ better than samples on a square grid.  If we try to match the solid line, we should expect the expansion coefficients to be very sensitive to the grid, and not to resemble $f(α_r)$.  If we try to do better than the truncation error, we will have to represent large number components, and the expansion coefficients will become very large.

\topinsert\XeTeXpicfile converge.pdf width \hsize
Convergence of a sampled vacuum state, as a function of the number of sample amplitudes.  The solid line shows sampling on a regular grid, the dashed one at random amplitudes.  Details in text.  The dotted line shows the difference between the expanded state and a vacuum state truncated to the extent of the grid; the norm of the truncation error is around $10^{-3}$.  The tension is the Frobenius norm of the matrix $T$ defined in the text.
\endinsert

The problem of expanding a state over coherent states is similar to expanding a vector over an overcomplete basis in the plane.  The main difference is that large sets of vectors in the plane are linearly dependent, while sets of coherent states are not.  The two approaches are illustrated below, with an expansion of the vector marked $+$ along the indicated directions.  The black lines show the expansion
$$ \vec x = π^{-1}∑_{r=1}^R (\vec u_r\cdot\vec x)\vec u.$$
As can be seen, this is a very good approximation even with five vectors; in fact, it is hard to visually distinguish the approximate point from $\vec x$ even with 3 directions.  The red lines show $\vec x$ expanded over the two directions closest to it.  The interesting thing is the size of the change of the expansion coefficients when $\vec x$ moves perpendicular to $\vec x$.

\topinsert\XeTeXpicfile circle-1.pdf 
\endinsert

To compute in phase space, we need to reduce the function $g$ to a finite vector of numbers.  This is usually done in one of two ways: by a vector
$$\vec g=\pmatrix{g(z₁) & ⋯ & g(z_R)},$$
where the points $z_i$ form a regular grid, or by
$$\vec g=\pmatrix{〈 u₁,g〉 & ⋯ & 〈 u_R,g〉},$$
where the distributions $u_i$ are a fixed basis.  The huge dimension of state space rules out any regular approach: we must sample $g$ at a set of irregularly spaced amplitudes or on an irregular basis, adapted to the support of $g$.

However, in these single mode numerical experiments, it is often useful to expand over a regular grid.  For example, this allows us to plot phase space functions.  It is convenient to normalise the samples so that $‖\vec g‖=‖A\vec g‖$.  If the sample amplitudes $α_r$ form a square gird with spacing $h$, the condition $\vec g_r=cg(α_r)$ is satisfied when
$$∑_r|\vec g_r|²=c²∑_r|g(α_r)|²=\int |g|²\,d²α≈h²∑_r|g(α_r)|²,$$
whence $c=h$, and the discretisation is
$$\vec g_r=hg(α_r).$$

This makes regularisation tricky.  The goal is to represent a state $|ψ〉$ as a superposition of coherent states, $|ψ〉=|A〉 c$.  We want the coefficients $c$ to correspond to a phase space function $g$ that is nearly analytic.  If the amplitudes $|A〉$ formed a regular grid , it would be obvious enough when $c$ was sampling an analytic function.  On an irregular grid, we would expect the coefficients to be larger where the grid is sparse, and smaller where it is dense.  It would be good to know if irregular coefficients are the result of sampling a smooth function on an irregular grid, or of sampling a wildly oscillating function.

The obvious way to attempt this is to factor the coefficients as $|ψ〉=|A〉 Wc'$, where $c'_i$ are samples that approximate $g(α_i)$, and $W$ is a matrix of weights that adapts the samples to the irregular grid.  In fact, we can do this exactly.  If we set $W=(〈 A|A〉)^{-1}$, then $c'=〈 A|ψ〉$.  We could try seeking coefficients $c$ such that the norm of $〈 A|A〉 c$ is minimal.

To avoid the complexities of Wirtinger calculus, the problem of relating the weights of gaussians to the values of a sampled function can be set up for a function of a single real variable.  It would be appealing for the weight matrix $W$ to be diagonal, so that the weight of the gaussian centred at each point was determined by the value of $f$ at that point.  This can be set up either as a Galerkin condition, or as a collocation condition: they turn out to be equivalent.  Unfortunately, the weights end up oscillating wildly.  This approach doesn't seem useful, but is presented for completeness, and because the phenomenon of adjacent weights cancelling keeps occuring, this being the simplest problem in which I've seen it.

As a collocation problem, consider the case where $f(x)=1$, and we wish to approximate $f$ by a function 
$$f'(x)=∑_i e^{-(x-x_i)²}w_ic_i$$
that satisfies the collocation condition $f'(x_i)=f(x_i)$.  The goal is for $c_i=1$, so we need ...

$$\pmatrix{1 & e^{-|α₁-α₂|²} &  & e^{-|α₁-α_R|²} \cr
	e^{-|α₂-α₁|²} & 1 &  & e^{-|α₂-α_R|²} \cr
	& & \ddots & \cr
	e^{-|α_R-α₁|²} & e^{-|α_R-α₂|²} & & 1}
\pmatrix{w₁²\cr w₂²\cr\vdots\cr w_R²}
= \pmatrix{π\cr π\cr\vdots\cr π}.$$

The Galerkin condition seeks a set of weights $w$ such that, for the normalised functions $u_i=π^{-½}e^{-(x-x_i)²}$, we have
$$1=\int u_i=∑_j w_j u_i(x_j)=∑_j w_je^{-(x_j-x_i)²}.$$
This is the same condition as the collocation condition.  The results are shown below.

Unfortunately, for random samples $A$, this tends not to have a solution: some of the $w_r²$ would have to be negative.  It looks like the amplitudes on the edges of the grid need large weights, because half of their corresponding gaussian lies outside the grid, and isn't sampled.  The gaussians centred inside the grid are sampled at all these points, and need negative weights at the inside points to compensate.  This sounds like the problem that Chebyshev points solve, where the function is sampled more closely at the edges of the domain.

There is another way to derive coefficients of the constant function, which might be more stable.  Suppose that $D$ is a subset of the real line, large enough that the tails of gaussians are neglibible outside it.  Let 
$$U=π^{-½}\pmatrix{e^{-(x-x₁)²}&⋯&e^{-(x-x_R)²}},$$
and $w$ be a column vector of $R$ real weights.  Then define a residual
$$r(w)=\int_D(Uw-1)².$$
The derivative of this with respect to the weights is
$$Dr(w)=\int_D 2(Uw-1)U=2\int_D U^{\rm T}Uw-U.$$
(Note that, for row vectors $a$ and $c$, and $b$ a column vector, $abc=c^{\rm T}ab$.)
Unfortunately,
$$\int (U^{\rm T}U)_{ij}=π^{-1}\int e^{-(x-x_i)²-(x-x_j)²}\,dx
	={1\over\sqrt{2π}}e^{(x_i-x_j)²/2},$$
and $\int u_i=1$, so this is the identical problem again.

An obvious way would be to assign amplitudes to the nearest sample.  That makes the volume for each sample a quite irregular prism; maybe there is a simple way to calculate the volumes of those, but I don't know it.  What if we assign a value to each point as an average over the sampled values, weighted, say, in proportion to the squared distances?  There would need to be some extra condition to ensure the function tends to zero at points remote from the samples.  Something like $x^{-2}e^{-|x|²}$ sounds plausible: it's quadratic at sort range, where the gaussian is flat, but outside the support of the samples, it decays in the right way for a phase space expansion.  I think the short range part needs to be a power law, so that it doesn't have a length scale built in when the distances between samples are random.  What's the best exponent?  I expect it depends on the dimension of the phase space, which would determine how many samples we expect to find close to a given one.


%% Sampling on random grids

\topinsert \XeTeXpicfile gweight.pdf width \hsize 
The weights $w_i$ required in order that the integrals of the functions $u_i(x)=e^{-(x-x_i)²}$ are given by the sums $∑_j w_j u_i(x_j)$.  The points $x_i$ are normally distributed (top), or evenly spaced (bottom).  Note how nearby points tend to cancel each others' samples.  I expect experts in Chebyshev sampling would not be surprised by this, but I am not yet such an expert.

The problem with the evenly spaced points is the gaussians at the edge of the grid: only half their support is covered by the grid, so that half needs to be doubly weighted.  This would overweight the gaussians just inside the grid, so the only solution is an oscillating one.  Similar things happen in the random ensemble, at the edges of a subgrid of closely spaced points.  Nearly the same thing happens when sampling with coherent states: the extreme amplitudes need extra weight, which must be cancelled by the interior ones.  When states are sampled on regular grids of coherent states, the outside amplitudes have very little weight in any case, and don't affect the solution much.  This only happens when the cloud of sampling amplitudes extends beyond the support of the distribution, and the outer amplitudes have very small weights.
\endinsert

The goal of fiding samples of the phase space distribution $f(α_r)$ is to regularise the expansion and inverse problems.  The expansion function with the least norm is guaranteed to be a nice function: we don't know much about the finite superposition whose weights have the least norm.

To the the norm of the function $f$, sampled on an irregular grid $α_r$, the obvious method is to interpolate.  Doing this might help with irregular weights due to irregular sample points, because a least-squares interpolant will average the weights at several adjacent points.  However, there is probably a better way to adjust the weights for irregular sampling.

We want to interpolate a function of a complex variable, that should be nearly analytic.  To me, the obvious way to do it is a least squares fit to the monomials
$$U=e^{-½|z|²}\pmatrix{1 & z & z* & z² & zz* & z^{2\ast} & z³ & z²z* & ⋯}.$$
It helps to think of $z^mz^{n\ast}=r^{m+n}e^{i(m-n)φ}$.  Note that if these monomials are orthogonalised by Gramm-Schmidt, in the order they are written, the functions $e^{-½|z|²}\pmatrix{1 & z & z² & z³ &  ⋯}$ are retained as part of the orthonormal set: $z^n$ is the first monomial proportional to $e^{inφ}$, so it is orthogonal to all those before it.  The same is true for $z^{n\ast}$, the first monomial proportional to $e^{-inφ}$.  So this basis is an obvious one for projecting onto analytic functions in $z$ or $z*$.  It will have the form
$$e^{-½|z|²}\pmatrix{1 & re^{iφ} & re^{-iφ} & r²e^{2iφ} & p_{11}(r) & r²e^{-2iφ} & r³e^{3iφ} & p_{12}(r)e^{iφ} & p_{21}(r)e^{-iφ}& ⋯},$$
where $p_{ij}$ is a polynomial of degree $i+j$.  Someone must have investigated these orthogonal polynomials before, but they don't appear in Courant \& Hilbert or in Abramowitz \& Stegun.

For least squares fitting, orthogonal polynomials are not necessary: the QR factorisation does that work for us.  To find norms, we do need to know the grammian matrix, whose elements are the Bargmann inner products of the monomials:
$$\eqalign{a_{mnpq}&=\int (z^{m\ast}zⁿ)*(z^{p\ast}z^q)e^{-|z|²}\,d²z \cr
	&= 2πδ_{m+n,p+q}\int_0^∞r^{m+n+p+q+1}\,dr \cr
	&= πδ_{m+n,p+q}\int_0^∞r^{m+n+p+q}\,2r\,dr \cr
	&= πδ_{m+n,p+q}Γ({m+n+p+q\over 2}+1)=π(n+p)!δ_{m+n,p+q}.}$$
Some fits of these to expansions of cat and coherent states over random amplitudes are shown below.  They suffer the usual problem that polynomial fits oscillate more wildly as the degree of the polynomial increases; there are meant to be ways to avoid that, but I'm not an expert in them.

\beginsection{Regularisation of the discrete expansion problem}
%% Picard plots

The most basic issue is the Picard condition.  This restates a simple observation: for the problem $\G \dot f=H|ψ〉$ to have a solution $\dot f$, the expansion of $\dot f$ over an orthonormal basis must have a finite norm.  If the basis is chosen as the right singular vectors $v_i$, this means that the sum of the squared moduli of their coefficients must be finite.  The component of $v_i$ is $〈u_i|H|ψ〉/σ_i$.  So the Picard condition is that, for the equation to have a solution, the series
$$∑_{i=1}^∞{\bigl|〈g_i|H|ψ〉\bigr|²\over σ_i²}$$
must converge.  The singular values reduce with $j$, by definition; this requires that the components of the left singular vectors reduce somewhat faster.

The variational problem seems to be pretty well-posed: if the initial superposition is the phase space function sampled on a grid, then the state that comes from a variational change in the amplitudes matches $H|ψ〉$ very closely.

%% Expansion tension

To capture the concept of a smooth expansion over an irregular set of nonorthogonal vectors, we'll define a quantity called the sampling tension.  Suppose that $|u〉$ and $|v〉$ are nearly parallel vectors, and the state $|ψ〉$ is expanded over a set that includes them, as $|ψ〉≈c_u|u〉+c_v|v〉+⋯$.  A relaxed expansion is one where $c_u|u〉$ and $c_v|v〉$ are as similar as possible; this can be ensured, regardless whether the $c$ are complex numbers of whether $|u〉$ and $|v〉$ are parallel or antiparallel, by requiring that the component of $c_u|u〉$ along $|v〉$ is equal to the component of $c_v|v〉$ along $|u〉$.  The difference between these components is
$$T_{uv}=\left| {c_v〈u|v〉\over ‖|u〉‖} - {c_u〈v|u〉\over ‖|v〉‖}\right|.$$
These differences form a tension matrix.  In the case that $|u〉$ and $|v〉$ are nearly orthogonal, their components should be independent.  In this case, the $T_{uv}$ will be small, because of the proportionality to $〈u|v〉$.  The tension matrix is a reasonable measure of the smoothness of an expansion: its Frobenius norm, the sum in quadrature of the components, would be a good summary.

The idea is that a relaxed expansion has all the coefficients pulling in the same direction, but an expansion with large tension is a tug-of-war, with adjacent coefficients pulling the expanded state in opposite ways.

The tensions were calculated for the grid and irregular expansions above, and plotted in the second graph.  Despite the much worse convergence of random sampling, the weights have very nearly the same tension as the regular grid.  

%% Tychonov conditioning

There is a family of methods for inverting operators with small singular values, known as regularisation methods.  A very useful idea is due to Tychonov.  The problem
$$\Gx f=|ψ〉$$
can be regarded as a least squares problem, where $|ψ〉$ is chosen to minimise the residual $‖\Gx f-|ψ〉‖²$.  The exact solution has residual zero, of course.  The problem is that it is dominated by components belonging to singular values of $\Gx$.  Suppose we modify this problem, and seek $|ψ〉$ to minimise $‖\Gx f-|ψ〉‖²+λ‖f‖²$, where $λ$ is a positive number, known as a Lagrange parameter for reasons that will become clear shortly.  There are two things to note here.  Firstly, this is still a least squares problem: it is equivalent to minimising 
$$\left‖ \pmatrix{\Gx \cr λ} f-\pmatrix{|ψ〉\cr 0}\right‖².$$
Therefore, it can be solved stably by the usual least squares methods.  Secondly, suppose $f_λ$ is the solution for a given value of $λ$.  If another function, $g$, had the same norm but a smaller residual, then $f$ would not minimise the Tychonov expression.  The Tychonov method is equivalent to constraining the norm of $f$, and seeking the solution with smallest residual given the constraint.  Or, equivalently, constraining the residual and seeking the solution with smallest norm.  This equivalence to a constrained optimisation problem is why $λ$ is a Lagrange parameter.

Solving the Tychonov problem for various values of $λ$ generates a family of solutions.  These can be plotted on a graph of residual against solution norm, where they mark the edge of the space of possible solutions.  Above and to the right of the curve, a solution can be found with the given norm and residual: below and to the left, it can't.

\topinsert\XeTeXpicfile lcurve-1.pdf\vskip 5mm\noindent\it L curve for a typical application of Tychonov conditioning\endinsert

An example for a typical application is shown above.  The context in which Tychonov regularisation is usually applied is that some observations $b$ are predicted by a physical state $x$, according to the linear law $Ax=b$.  The real state is $x₀$, and the observed data are $b=Ax₀+e$, where $e$ consists of measurement error.  The problem $Ax=b$ is solved for $x$, and regularised with various Lagrange parameters.  At the top left, the norm of $x$ is constrained to be zero, so the solution is $x=0$, and the residual is the norm of $b$.  As the constraint on the norm is relaxed, the solution becomes closer to $x₀$, and the residual decreases.  At the elbow of the curve, the norm of the solution is constrained to be close to the norm of the real state $x₀$, the solution is close to $x₀$, and the residual is the norm of the measurement error.  Relaxing the constraint further allows spurious solutions $x$ that fit the measurements better than the real state $x₀$ does.  Typically, they won't fit much better, so the residual doesn't decrease much below $‖e‖$.  However, the solution now includes components belonging to small singular values, and the norm of the solution will increase as far as it is allowed to.

%% ill-posedness

As shown above, the restricted expansion operator $\Gx $ has singular values that decay to very small values, whose singular vectors are states with particle numbers corresponding to coherent amplitudes outside the support of $χ$.  

There are two types of ill-posed problems, rank-deficient and discrete.  They are distingished by how the singular values decay to zero.  In a rank-deficient problem, there is a sharp cut between large and small singular values.  The large ones are large enough that components in their singular directions do not contribute excessively to the solution, the small ones would contribute huge components, but they are small enough to round to zero, and ignore the component along that singular direction.  In discrete problems, the singular values decay continuously.  The size of the components of the solution varies continuously: there is no point where they become huge, but a continuous minimum and a gradual rise as the singular values get smaller.

I've been assuming that the variational problem is discrete, because it comes from a discretisation of an integral equation.  However, the integral equation is a complex convolution, and for some values of the complex amplitude, the kernel oscillates arbitrarily sharply.  This is the same sort of integral equation as the Fourier transform, where we know $f(t)$, and want to find $F(ω)$ such that
$$f(t)=\int F(ω)e^{iωt}\,dω.$$
This also has a kernel that oscillates at a rate proportional to $ω$, and it also has very stable solutions.

A numerical example is shown below.  The state $|ψ〉$ is the Schrödinger cat that a coherent state becomes after half a cycle in a quartic oscillator; the amplitude of the initial coherent state was $2$.  A set of 40 sample amplitudes $α_r$ was drawn from the probability distribution $P(α)=|e^{-½|α|²}〈G⁺|ψ〉(α*)|²$.  These are plotted below, as circles in phase space.  For ensembles with fewer components, the results were very inconsistent between draws.

The state $|ψ〉$ was expanded in the form $|ψ〉=|A〉\vec f$, where $|A〉=\pmatrix{|α₁〉&⋯&|α_R〉}$, and $\vec f$ is a column vector of $R$ complex weights.  This was done with Tychonov regularisation, for various Lagrange parameters: these and the residuals are shown with the graphs.  The complex weights $c$ are indicated by brightness and color as above.  Note that least norm regularisation has the desirable property that nearby $α_r$ have similar $c_r$: the least norm solution to $a+b+c=X$ is $a=b=c=X/3$.

\topinsert\XeTeXpicfile cats.pdf width \hsize
\vskip 5mm\it A cat state expanded over a random set of 40 amplitudes, as described in the text.\endinsert

A basic question is why regularisation helps.  Usually, it's applied to the inversion of measured data, where the right hand side of the least squares problem includes noise.  The aim is to prevent amplified noise from dominating the solution.  In our problem, the right hand side is calculated instead of measured, and the only noise is rounding errors.  Those are 16 orders of magnitude smaller than the exact right hand side, so rounding the small singular values up to $10^{-10}$ should be more than adequate.  We've needed much stronger regularisation than that: therefore, rounding errors aren't the problem.

The only other error I can think of is nonlinearity.  Tychonov parameters of order $10^{-10}$ do work for the linear expansion problem.  Perhaps we're not doing regularisation, we're actually establishing a trust region, where the linear least squares problem is a good approximation to the actual change in $|ψ(z)〉$.

If that's the case, it's tempting to make the weights linear instead of logarithmic, so that that part of the problem is exactly linear.  However, the function $e^φ$ is pretty nearly linear in the range we're shifting $φ$, so that can't help much.  In fact, the whole problem is quite linear.

To avoid the amplitudes clustering, I need to find out how good a fit is possible with evenly spaced ampltiudes, and set the regularisation parameter so that the residual is on that order.  In fact, I already know how large the phase space expansion should be: the function being sampled should have a norm around 1.

The Picard condition is very obviously satisfied by the projection operator $|Γ〉$.  The left singular vectors can be any orthonormal basis of ket space; the singular values are all 1, so they don't reduce at all, and any ket has a preimage with the same norm as itself.  Functions in the null space of $|Γ〉$ don't have components in the left singular directions, so, if the problem is solved by expanding over singular vectors, they will not be present in the solution. 

\topinsert\XeTeXpicfile catpicard.pdf width \hsize\vskip 5mm\it
Picard plot for cat expansion. \endinsert

The next Figure is a Picard plot for the Galerkin discretisation $〈A|A〉\vec f=〈A|ψ〉$, where $\hat f$ is a vector of weights such that $|A〉\vec f$ approximates $|ψ〉$.  The singular value decomposition is $|A〉=|U〉SV$.  For the kets to be orthogonal, $|u_r〉$ must have support close to $n=r$.  The components of the cat have coherent amplitude 2, so their largest number component is $n=4$, and the components drop quickly as $n$ increases further.  Above $r=15$, the singular ket $|u_r〉$ has very small components on these number states, so it hardly overlaps $|ψ〉$.  At the same time, the singular kets start to have particle numbers too large to be consistent with the sample amplitudes $α_r$, so the singular values decrease.  This is shown in the plot of Fock state components of the singular kets.  In fact, they decrease more rapidly than the components of $|ψ〉$, and the discrete Picard condition is not satisfied: the discrete doesn't have a finite solution.  Seeing that the continuous operator $\G$ is perfectly conditioned, this is a very bad discretisation.

The final graph is an L curve when the discrete problem is Tychonov regularised.  It shows the expected shape, with minimum residual around $2×10^{-8}$.

\topinsert\XeTeXpicfile catsws.pdf width \hsize\endinsert

\topinsert\XeTeXpicfile catl.pdf width \hsize\endinsert

A common observation when states are expanded over an irregular grid is that, when two sample amplitudes are close together, the weights tend to be large, and cancel out.  This is not at all surprising when one considers that the coherent states $|α〉$ and $|α+h〉$ become nearly parallel in Hilbert space as $h→0$.  The figure demonstrates why weights of nearly parallel (or antiparallel) vectors tend to cancel out, and the second figure shows how this looks in phase space.

\topinsert \XeTeXpicfile geometry-1.pdf
\it A 2 dimensional section through Hilbert space, where fixed vector $|Ψ〉$ is expanded as $|Ψ〉=a|α〉+b|β〉$.  As $|β〉$ becomes more parallel with $|α〉$, the components grow and cancel.  This occurs for all vectors outside the shaded segments, which shrink to nothing when $|α〉$ becomes parallel to $|β〉$.
\endinsert

\topinsert \XeTeXpicfile cancel.pdf width \hsize
\it The vacuum state was expanded over two coherent states, with amplitudes $±h$.  The amplitudes and the entire expansion of the least-squares fit are plotted in the first row of images.  The norm of the expansion vector, and the residual of the expansion, are graphed above as black lines.  As the amplitudes converge, the residual decreases—the leftmost phase space plot is very nearly exact—while the norm of the expansion stays around 1.  This coincides with the amplitudes passing through bright areas of the images, where the entire expansion has support.

The second row of images, and the red lines on the plot, show similar expansions of the normalised state parallel to $|0.1〉-|-0.1〉$.  This is almost exactly parallel to the derivative of the vacuum state with respect to the real part of its amplitude.  Again, the leftmost image is nearly an exact expansion.  This time, the entire expansion does not have support around $α=0$.  As the expansion amplitudes converge, the residual decreases, but the norm of the expansion vector explodes.  As explained in the text, the distance between two small amplitudes in phase space is very nearly the angle between the ket vectors for the corresponding coherent states.  Thus this is exactly the expansion depicted in the last figure.

If the two sample amplitudes were part of an irregular expansion, and there happened to be no other sample amplitudes nearby, a small component along the direction of the second state would dominate the expansion coefficients at these points.  But, given those sample amplitudes, that would be the only way to represent such a component.
\endinsert

\beginsection{Adapting the sample amplitudes to preserve condition}

I'm getting a picture of how the final algorithm will look.  At each timestep, the directions belonging to large singular values of $|Dψ〉$ will be used for variational dynamics.  This can be done by solving at Tychonov conditioned problem, for example.  At the same time, the directions with small singular values will be used to keep the discretisation stable.

To solve our differential equation, we need to solve an iterated least squares problem.  We need to do so in a way that preserves the condition of $|A(z)〉$.  When we shift $z$ in the direction $\dot z$, how does the condition change?  The issue is the small singular values, so it makes sense to constrain the smallest singular value $σ_R$.  This is a function of $z$.  We proceed to calculate its derivative by first order pertubation theory.

Let 
$$|A(α)〉=\pmatrix{|α₁〉& \cdots &|α_R〉}
	=\pmatrix{\cdots &e^{-½|α_r|²+α_ra†}&\cdots}|0〉.$$
This is not an analytic function, so there is no Cauchy derivative such that $|A(z+h)〉=|A(z)〉+|DA(z)〉h+O(h²)$.  There are Wirtinger derivatives such that $|A(z+h)〉=|A(z)〉+|DA(z)〉h+|CA(z)〉h†$.  However, in this case it is easier to work with real and imaginary parts, defining
$$|A'(x,y)〉=\pmatrix{|x₁+iy₁〉&\cdots &|x_R+iy_R〉}
	=\pmatrix{\cdots &e^{-½x_r²-½y_r²+(x_r+iy_r)a†}&\cdots}|0〉.$$
The partial derivatives are easily determined to be
$$|∂_rA'(x,y)〉=\cases{
	\pmatrix{0 & (-x_r+a†)|x_r+iy_r〉 & 0} & $1≤ r≤ R$, \cr
	\pmatrix{0 & (-y_r+ia†)|x_r+iy_r〉 & 0} & $R<r≤ 2R$.}$$
Because $|A〉$ matrix-valued, its total derivative is some kind of third rank tensor, and its partial derivatives are simpler to work with.  The total derivative $Dσ_R$ is just a row vector, and will be used often.

Let the singular value decomposition of $|A(α)〉$ be $|A(α)〉=|U〉SV†$.  The standard ways to compute singular values use the matrix
$$M(x,y)=\pmatrix{0 & |A'(x,y)〉\cr 〈A'(x,y)| & 0}.$$
Here, $|A(x,y)〉$ maps weight vectors in $C^R$ to kets, while $〈A(x,y)|=VS〈U|$ maps kets back to column vectors of inner products.  The operation of the zeros is obvious; formally, they need to be a row of null kets and a column of null bras.  The matrix $M(x,y)$ operates on a vector comprising a ket above $R$ complex numbers.  Now, the vector
$$x={1\over \sqrt2}\pmatrix{|u_R〉 \cr v_R}$$
is a normalised eigenvector  of $M$, belonging to the eigenvalue $σ_R(x,y)$.  As is well known in first-order pertubation theory, the derivative of this eigenvalue with respect to $x$ and $y$ is given by
$$∂_rσ_R=x†(∂_rM)x
	=½\pmatrix{〈u_R| & v†_R}\pmatrix{0 & |∂_rA'(x,y)〉\cr 〈∂_rA'(x,y)| & 0}\pmatrix{|u_R〉 \cr v_R}=\Re\left(〈u_R|∂_rA〉v_R\right).$$
Substituting the partial derivatives gives
$$∂_rσ_R=\cases{\Re\left(〈u_R|(-x_r+a†)|α_r〉v_R^r\right) &  $1≤ r≤ R$, \cr
	\Re\left(〈u_R|(-y_r+ia†)|α_r〉v_R^r\right) & $R<r≤ 2R$.}$$

The accuracy of the pertubation method is shown below.  A vector of 5 amplitudes was drawn from a normal distribution around amplitude 2, shown in the upper plot.  The lower plots show how the smallest singular value of $|A〉$ varies as the amplitudes shift.  The five columns show changes in the five amplitudes.  The black lines show how $σ_R(x,y)$ varies with the real part $x$, the red lines how it varies with $y$.  The dotted lines show the linear approximation derived above by pertubation theory, and the second row of plots show the error in the approximation.  The linear approximation is good; presumably it holds for changes small compared to the separation of the amplitudes.

\topinsert 
\vskip -40mm
\XeTeXpicfile psfa.pdf width \hsize
\vskip -90mm
\XeTeXpicfile psv.pdf width \hsize\endinsert

Given the gradient $Dσ_R$, we wish to find a direction where $σ_R$ increases most rapidly for a given change in $|ψ〉$.  Given a small change $|dψ〉$ lying in the tangent plane, the corresponding change in $z$ is given by $dz=〈Dψ^{+}|dψ〉$, where $〈Dψ^{+}|$ is the pseudoinverse of $|Dψ〉$.  The change in $σ_R$ is $dσ_R=Dσ_Rdz=Dσ_R〈Dψ^{+}|dψ〉$.  So the gradient direction in Hilbert space is $|Dψ^{+}〉Dσ_R†$; the corresponding direction in $z$ space is $〈Dψ^{+}|Dψ^{+}〉Dσ_R†$.  In terms of the SVD $|Dψ〉=|U〉SV†$, this is $〈U|S^{-2}|U〉Dσ_R†$.  Here, $|ψ〉$ is considered a function of $4R$ real variables instead of $2R$ complex ones.

There are now three goals for optimisation of $\dot z$:
\item{1.} The ket $|ψ(z)〉$ solves Schrödinger's equation.  I.e., $|Dψ(z)〉\dot z=H|ψ(z)〉$.
\item{2.} Singular vectors of $|Dψ〉$ belonging to small singular values do not dominate the solution.  This is controlled by a Tychonov condition, $\dot z=0$, or perhaps by truncation.
\item{3.} The condition of $|A(z)〉$ improves.  This can be set up as a least squares condition $Dσ_R(z)\dot z+Cσ_R(z)\dot z*=|σ_R||\dot z|_{\rm opt}$, given a desired rate of change of $z$.  No doubt there is a better way.
\item{4.} There might need to be fourth condition, to prevent amplitudes with small weights from wandering off to infinity.

Putting these conditions together, with regularisation parameters $λ$ and $ε$, gives the least squares problem
$$\pmatrix{|Dψ(z)〉& 0 \cr λI & 0 \cr Dσ_R(z) & Cσ_R(z)}
	\pmatrix{\dot z\cr \dot z*} ≈
	\pmatrix{H|ψ(z)〉\cr 0\cr |σ_R||\dot z|_{\rm opt}}.$$
This raises a technical problem, because we need to constrain $\dot z*$ to be the conjugate of $\dot z$.  In the space of complex numbers, this constraint is not linear, because $(cz)*≠cz*$.  However, that is true if $c$.  Moreover, if $z=x+iy$, then $|z-w|²=|x-\Re w|²+|y-\Im w|²$, so a least squares problem in $z$ has the same solution as the equivalent least squares problem in $x$ and $y$.

Consider the problem 
$$ \pmatrix{A & B}\pmatrix{z\cr z*}≈c,$$
with $z∈C^m$, $c∈C^n$, and $A,B∈C^{m×n}$.  If $z=x+iy$, the left hand side can be rearranged
$$\pmatrix{A & B}\pmatrix{z\cr z*}
	=\pmatrix{A & B}\pmatrix{I&iI\cr I&-iI}\pmatrix{x\cr y}
	=\pmatrix{A+B & iA-iB}\pmatrix{x\cr y}.$$
Separating the rows into real and imaginary parts gives
$$\pmatrix{\Re (A+B) & \Re(iA-iB)\cr\Im(A+B) & \Im(iA-iB)}\pmatrix{x\cr y}
	=\pmatrix{\Re A+\Re B & \Im B-\Im A\cr\Im A+\Im B & \Re A-\Re B)}
		\pmatrix{x\cr y}≈\pmatrix{\Re c\cr \Im c}.$$

Below is a plot of the condition number of $|Dψ(z)〉$ for a superposition, as the value of $z$ varies by $0.1$ in every direction.  The top line shows the effects of changing $φ$, the bottom line $α$, components are shown left to right.  Changes to real parts are black, imaginary parts red.  There are a few things to note.  The condition is very bad, with number $1.4×10^{11}$.  This is entirely due to the similar amplitudes of components 1 and 3: those graphs are on a different scale to the others, and it's logarithmic.  As they move closer and become identical, the condition number increases $10^{14}$.  No doubt it would go higher if we sampled the graphs more finely, but this is huge—reciprocal floating point epsilon is $5×10^{15}$.  If the separation between these components is doubled, the condition number improves by an order of magnitude.

\centerline{\XeTeXpicfile j1.pdf width \hsize}

\centerline{\XeTeXpicfile j2.pdf width \hsize}

The top line indicates that the condition number is independent of the phases of the components.  I didn't forsee that, but it isn't too surprising: we can change the coefficients of the superposition in any direction we like, regardless of their current values, and the derivative is only sensitive to changes.

\beginsection{Ways the problem has been simplified for this paper}

This paper will talk about two related but distinct inverse problems.  Both start with a given wavefunction $|ψ〉$, and a set of amplitudes in phase space, $\{α_j\}$, over which to expand it.  In systems with few modes, these points could be a grid, but in many-mode systems they will have to be more sparse than that: there isn't space to store grid coefficients.  The first problem, let's call it the inverse problem, is to find a set of weights $f_j$ such that $∑f_j|α_j〉$ approximates $|ψ〉$.  This is simply finding a discrete inverse to $G$ sampled on the grid $α_j$; since $G$ is a projection, the pseudoinverse of $G$ should solve it well enough.  The differential case, where we have an expansion for $|ψ〉$ which we want to perturb to represent $|ψ〉+|dψ〉$, is no harder.  Because $G$ is linear, we can expand $|dψ〉$, and add it to the expansion of $|ψ〉$.

The second problem, the variational one, is a generalisation.  As well as the grid $α_j$, we have a set of weights $f_j$ representing some wavefunction, presumably not $|ψ〉$.  Instead of expanding $|ψ〉$ over phase space, we want to expand it over the columns of $|Dψ(f,α)〉$, the set of all changes we can make by slightly adjusting the original expansion, and the grid.  Obviously, we can solve this problem simply by setting the coefficients of $|∂fψ(f,a)〉$ to the solution of the inverse problem, and those of $|∂fψ(f,a)〉$ to zero.  Is this the smallest norm solution?  Probably not: if a coherent state was moving in an oscillator, it might be easier to just rotate the $α_j$.

Since we have a totally stable solution to the problem, why would we try anything else?  Because we're iterating the problem, and we need to preserve our ability to solve it at future steps.  If the amplitudes form a grid, which covers all the energies the system could possibly access, we're set: we'll always be able to expand over them.  However, we can't do that for many mode systems: there will always be accessible states of the system whose support in phase space is remote from the current grid.  The set $|Dψ〉$ will have some small singular values, corresponding to directions where the amplitudes and samples are adjusted in a way that preserves the wavefunction.  We are free to add these as we like, in order to maintain a grid on which we can expand $|ψ〉$ in the future.

%% Fock space is cheating

Expanding the quartic oscillator over Fock states allows it to be solved very stably, and as precisely as desired by rotating the phases of the expansion coefficients.  Also, it is easy to find the expansion over Fock states of a superpositon of coherent states.  This means that the stability of time discretisation can be removed from the problem entirely.  The state can be propagated exactly, and the coherent states adjusted to fit.  The most direct way to do so is by nonlinear optimisation at every timestep; Matlab provides the necessary routines in a convenient package.  Unfortunately, this is not at all stable.  

In a sense, expanding over Fock states is cheating.  If it were practical to do this, there would be no point using a variational method: these would be employed in many-particle problems, where an orthonormal basis such as the Fock states would be unmanagably large.  In principle, the operations required for the second and third methods could be performed on the superpositions of coherent states, using the normal equations, without projection on an orthonormal basis; in a many-particle problem, they would have to be.  However, this would raise the question whether those least-square solutions from the normal equations were stable.  By using an orthonormal basis, we can be more confident of this.

To do this, we need some bracket matrices.  We have
$$|Dψ(φ,α)〉=(1,a†)|ψ(φ,α)〉.$$
The brackets with the number states are
$$〈m|H/\hbar|n〉=n(n-1)δ_{mn}$$
and
$$〈n|Dψ(φ,α)〉=e^φ\pmatrix{{α^n\over \sqrt{n!}} &  α^{n-1}\sqrt{n\over (n-1)!}},$$
as is the bracket of the quartic oscillator Hamiltonian $a^{2\dagger}a²$
$$〈n|H/\hbar|ψ(φ,α)〉=e^φα^n\sqrt{n(n-1)\over(n-2)!}.$$


The instability in the dynamics is caused by the near-singularity of $|Dψ〉$.  If a singular value decomposition $|Dψ〉=|U〉SV†$ is taken, the singularity is indicated by small singular values on the diagonal of $S$.  It is often measured by the condition number of $|Dψ〉$, the ratio of the largest to the smallest singular value.  Let $C(z)$ be the condition number of $|Dψ(z)〉$.  As part of the regularisation process, it would be useful to avoid changing $z$ in directions that increase $C(z)$.  For example, if two coherent states had real amplitudes, and the real part of the expected amplitude needed to increase, it would be done by shifting the right hand one away from the left hand one, not the left hand one closer to the right hand.  Is there a way to make a computer do that?

The least squares problem can be solved using the singular value decomposition: to approximate a state $|h〉≈|Dψ(z)〉dz$, we set $dz=VS^{-1}〈U|h〉$.  The first problem is that some of the $σ_i$ on the diagonal of $S$ will be small, because the $|u_i〉$ that correspond to them change different components of the superposition $|ψ(z)〉$ in ways that nearly cancel out.  So a error in the discrete representation of $|h〉$ in the direction $|u_i〉$ will cause a very large error in the least squares $dz$ in the direction $v_i$.   The way to avoid such large errors is to shift the small singular values of $S$ up to a lower bound $ε$, and thus reduce the large singular values of $S^{-1}$ below an upper bound $ε^{-1}$.  There are many and varied ways of doing that. 

\beginsection{Regularising the inital fit equation}

As discussed below, the time-dependent Schrödinger equation for a quartic oscillator is numerically difficult, no matter how it is discretised.  The following problem aimed to solve an intrinsically simple differential equation, 
$$ {d\over ds}|ψ(z(s))〉=|ψ₀〉-|ψ(z(s))〉.$$
Here, $|ψ₀〉$ was the half cycle cat state; $|ψ(z)〉$ had 15 components, and was initialised by sampling random, and linearly fitting weights.  The result is shown on the left below.  The clusters of complementary colours indicate components that nearly cancel each other out; these should not be present when the condition of $|A〉$ is constrained, but are expected from random sampling.  They might also indicate too small a Tychonov parameter: large parameters will tend to give nearby points equal weights.

\topinsert \XeTeXpicfile expens.pdf width \hsize \endinsert

The equation was solved, in the script {\tt hzero.m}.  The results are shown below, and the final superposition is plotted on the right above.  The second graph below isthe smallest singular value of $|A〉$.  Many draws of initial samples give the exponential decay one would expect.  This one gave two surprises: initially, the distance between $|ψ〉$ and $|ψ₀〉$ increases with time.  This is entirely wrong, according to the differential equation.  The initial state was a good approximation to $|ψ₀〉$, so there might not be much scope to improve the approximant.  However, it should certainly not move away from the target.

The variational method was the least distance in Hilbert space, with Tychonov regularisation.  Time was discretised by Euler's formula, which should solve exponential decay very stably.

The second surprise is at large $s$, where the approximant improves, but so does the condition of $|A〉$.  In most draws of initial samples, the approximant improves by clustering the amplitudes towards the states $|2i〉$ and $|-2i〉$, which makes the conditioning worse.

\topinsert \XeTeXpicfile exp.pdf width \hsize \endinsert

There are a few things to note about the initial trajectory.  The initial weights already satisfy a least squares condition.  Therefore, the intial change in the superposition should be entirely in the amplitudes: the weights are already optimal.  If that isn't the case, the regularisation of the expansion problem would have to be inconsistent with that of the variational problem, so that we're accepting variational changes that we previously rejected when we found the initial state.

\beginsection{Quantum dynamics in Hilbert space}

If Hilbert space is considered as a real vector space, complex multiples of the eigenstates of the Hamiltonian form planes instead of lines.  The eigenstates evolve as $e^{iωt}$, so, when the dynamics are projected onto an eigenplane, the state traces out a circle, with radius equal to the absolute value of the the coefficient of the eigenstate when the initial state is expanded.  Different eigenstates circle the origin with different frequencies.

The semi-implicit formula has a special property: when the trajectory of $u(t)$ is a circle, the $vⁿ$ lie exactly on that circle, and the discretisation error is equivalent to rescaling time by a factor that depends on the frequency and the discrete timestep.  In Hilbert space, this means that the projections of the state onto eigenplanes still circle the origin exactly: only the frequencies are altered by discretisation.  So the effect of discretising quantum dynamics using the semi-implicit formula is merely to shift the energies of the eigenstates.  Of course, this can be a significant effect: it's the only difference between the harmonic and quartic oscillators.

%% Discrete time and stiffness

I used three discretisation formulae for ODE, with different regions of stability.  As a first step, the Hamiltonian and a coherent state with real amplitude 2 was expanded over Fock states $|0〉$ through $|10〉$, and the resulting equations for the components solved directly.  The Hamiltonian is
$$H=\hbar a^{2\dagger}a²,$$
so the Fock states are eigenstates, although the spacing of their energies is uneven.  The quantum state at time $t$ was expanded over a truncated Fock basis $|N〉=\pmatrix{|1〉 & |2〉 & ⋯ & |N〉}$, with a column vector of coefficients $c(t)$, as
$$|ν(c(t))〉=|N〉c(t).$$
Schrödinger's equation expands to
$$|N〉Dc(t)=|D(ν\circ c)(t)〉={H\over i\hbar}|ν(c(t))〉=|N〉〈N|{H\over i\hbar}|N〉c(t),$$
whence
$$Dc(t)=-i〈N|{H\over \hbar}|N〉c(t).$$
Usually, these expansions would be approximate due to basis truncation.  In this case, the Fock states diagonalise $H$, so they are exact.  However, in general the initial state can only be approximately expanded over $|N〉$.  Time was discretised with the explicit Euler formula, which replaces the differential equation
$$Du(t)=f(u(t))$$
with a difference equation
$$v^{n+1}=vⁿ+τfⁿ,$$
where $tⁿ=nτ$, $vⁿ$ is meant to approximate $u(tⁿ)$, and $fⁿ=f(vⁿ)$.  Let
$$vⁿ=cⁿ\qquad{\rm and}\qquad fⁿ=-i〈N|{H\over\hbar}|N〉c(t).$$
Since $〈N|H|N〉$ is diagonal, the matrix product can be performed sparely as a dot product.  The results are shown below.  The Fock states are eigenstates of the Hamiltonian, so their amplitudes should be constant: in the exact solution of Schrödinger's equation, all the ratios plotted are exactly 1.

%\centerline{\XeTeXpicfile dry.pdf width \hsize}

These results are the textbook signs of stiffness.  As the time step is reduced from values on the order of 1, the discretised solutions diverge hugely, before converging at very small time steps.  In the standard theory of discrete approximations to $Du(t)=f(u(t))$, stability is determined by the eigenvalues of the linear transform $Df$.  In Schrödinger's equation, $f$ is the Hamiltonian, a linear transform, so in our expansion, $df=f=-i〈N|H/\hbar|N〉$, and the relevant eigenvalues are $-iEⁿ/\hbar=-in(n-1)$.  The stiffness of a differential equation is often measured by the ratio of the sizes of these eigenvalues.  For the quartic oscillator, some eigenvalues are zero, so this is not a sensible measure.  However, the largest eigenvalue, which often determines the timestep, increases quadratically with the size of the Fock basis as $N(N-1)$.

The stability region of the explicit Euler formula is the disk $|\bar k-(-1)|≤1|$.  So it is unstable for any Jacobian with complex eigenvalues, which is certainly the case for a Hamiltonian.  The implicit Euler formula, 
$$v^{n+1}=vⁿ+τf^{n+1},$$
has the opposite stability region.  For the oscillator, this becomes
$$c^{n+1}=cⁿ-iτ〈N|{H\over\hbar}|N〉c^{n+1},$$
so $c^{n+1}$ satisfies the linear equation
$$(1+iτ〈N|{H\over\hbar}|N〉)c^{n+1}=cⁿ.$$
This is still diagonal for the quartic oscillator.  To order $τ$, the implicitn formula is the same as the explicit one.  Obviously the higher powers of $τ$ make the difference when $τ$ is finite.

The results of the backward Euler formula are the opposite of the Euler formula.  Instead of increasing, the amplitudes of the larger number states decrease nearly to zero.  This distorts the trajectory of $〈a(t)〉$, as show below.

A third method is the semi-implicit one, with the formulae
$$v^{\prime n}=vⁿ+½τf^{\prime n}\qquad v^{n+1}=vⁿ+τf^{\prime n}.$$
With the Hamiltonian, this becomes
$$c^{n+1}=cⁿ-iτH\biggl(1+{iτ\over 2}H\biggr)^{-1}cⁿ.$$

The solutions from method 3 provide a test of the stiffness of the ODEs defined in parameter space by the least squares problem.  The step in Hilbert space is very nearly the same from one time step to the next.  If the step in parameter space changes dramatically, this would suggest that the variational solution is very sensitive to the state, and thus to some of the parameters.  This would indicate that the ODEs are stiff.

\beginsection{Quantum dynamics in phase space}

The following graphs show a coherent state, with real amplitude 2, propagating in a quartic oscillator potential.  The propagation was done exactly, in a Fock space truncated at $|19〉$; at times $π/10, π/5, 3π/10,\ldots,π$, a 10 component superposition was fit to the state by the routine {\tt cohfit5}.  The SVD of $|Dψ〉$ was computed for each of these superpositions, and the Picard condition investigated.  It appears that this discretisation is inherently ill-posed.

\topinsert \XeTeXpicfile spsns.pdf width \hsize
Amplitudes of components \endinsert

\topinsert \XeTeXpicfile traj.pdf width \hsize
Left: expectation value $〈a〉$ at sampled times.  Right: residual of fit by superposition (solid) and condition number of Jacobian of fitted superposition (dotted) \endinsert

\topinsert \XeTeXpicfile picard.pdf width \hsize
Picard plots at fitted times.  Circles are singular values, crosses components of LSVs, red stars components of RSVs.  The discrete Picard condition is clearly not satisfied. \endinsert

\topinsert \XeTeXpicfile svnum.pdf width \hsize
Expected particle number for left singular vectors.  The svs with small sws have large particle number.  When $H$ is quartic, these have large components in $H|ψ〉$.  In order for the svs to be orthogonal, the points are likely to lie around the line ${\rm sv}=n$. \endinsert

In the second choice, the state is approximated as a superposition of coherent states,
$$|ψ(z(t))〉=∑_{φ,α∈z(t)}e^{φ+αa†}|0〉=∑_{φ,α∈z(t)}e^{φ+½|α|²}|α〉.$$
Time is discretised immediately, setting
$$vⁿ=|ψ(zⁿ)〉\qquad{\rm and}\qquad fⁿ={H\over i\hbar}|ψ(zⁿ)〉,$$
so that the midpoint formula becomes
$$|ψ(z^{n+1})〉≈|ψ(z^{n-1})〉+{2τH\over i\hbar}|ψ(zⁿ)〉.$$
The approximation is due to basis truncation, as always in a least squares sense.  Writing the unknown $|ψ(z^{n+1})〉$ in terms of $|Dψ(zⁿ)〉$ and expanding over a Fock basis gives
$$ 〈N|Dψ(zⁿ)〉\bigl(z^{n+1}-zⁿ\bigr)≈〈N|ψ(z^{n-1})〉-〈N|ψ(zⁿ)〉-2iτ〈N|H|ψ(zⁿ)〉.$$
This matrix equation can be solved for $z^{n+1}-zⁿ$, which can be used to update $z$.  

The third choice, the conventional variational one, is to discretise the state first, setting
$$vⁿ=zⁿ\qquad{\rm and}\qquad |Dψ(zⁿ)〉fⁿ≈{H\over i\hbar}|ψ(zⁿ)〉.$$
The midpoint formula is used exactly, and $fⁿ$ satisfies the least squares problem
$$〈N|Dψ(zⁿ)〉fⁿ≈-i〈N|Dψ(zⁿ)〉{H\over \hbar}|ψ(zⁿ)〉.$$
Usually, this would be expanded over $〈Dψ(zⁿ)|$ to give a set of normal equations.  However, when $|Dψ(zⁿ)〉$ is near singular, the numerical stability of the normal equations is suspect.  Expanding over an orthonormal basis removes this confounding instability.

\topinsert \XeTeXpicfile flwc0n10.pdf width \hsize \endinsert

\topinsert \XeTeXpicfile flwc0n5.pdf width \hsize \endinsert

The above plots show my first attempt at this.  The initial state was coherent, with real amplitude 2.  A superposition of 5 coherent states was initialised with amplitudes normally distributed around 2, the norm of the superposition being 1, and the components having equal norms.  This superposition was fit to the coherent state by minimising their distance in Hilbert space; for stability, the weights were constrained so that no component had a norm less than 0.1.  This was done by the Matlab routine {\tt fmincon}.  The initial state was truncated to Fock states $|0〉$ to $|10〉$, and propagated exactly in a Hamiltonian $\hbar a^{2\dagger}a²$ with timestep 0.03.  After each step, the fitting of the superposition to the exact state was repeated.

The results are shown in the first Figure.  The left hand plot shows the complex amplitude of the exact solution in black, and the fitted solution in blue.  The right hand plot shows the residual of the fit as a solid line, and the condition number of $|Dψ(z)〉$ at the fitted $z$ as a dotted line.  The program stopped after a short time, when the fitted amplitudes caused overflow when expanded over Fock coefficients.  

The second Figure shows the same simulation, with the states expanded over Fock states $|0〉$ to $|5〉$.  This has a significant effect on the truncated exact solution, adding an extra loop to the amplitude.  The fitting works a bit longer, but still fails at a very short time.

\centerline{\XeTeXpicfile flwc2n10.pdf width \hsize}

\centerline{\XeTeXpicfile flwc2n5.pdf width \hsize}

The next two plots show a different fitting strategy.  Instead of constraining the weights, a cost term was added in quadrature to the Hilbert space distance, proportional to the logarithm of the condition number of $|Dψ(z)〉$ for the fitted $z$.  The constant of proportionality was $10^{-5}$; this was adjusted until a good fit was obtained.  The most apparent thing is that the fit is much more stable: this is being done with timestep 0.1 instead of 0.3, but the fit stays stable for the whole period.  (I had to redraw the initial amplitudes a few times before that happened.)  It isn't a very good fit through the middle of the period: the residual increases to order 1.  The really interesting thing is what happened when, by accident, I truncated the Fock basis at $N=5$.  As before, this perturbs the trajectory in Hilbert space.  However, the perturbed trajectory can be fit very well: the residual is between $10^{-2}$ and $10^{-5}$, and this is done with quite well conditioned superpositions.

So even without stiffness in a discrete time propagation, there is something in the high energy components of $H|ψ〉$ that makes fitting the state with coherent states unstable.  This isn't too surprising: these high energy components are rapidly oscillating, and will by magnified when the inverse problem is solved with fairly smooth, low amplitude coherent states.  To fit the oscillating components, we need coherent states with large amplitudes, and low weights.  But the low weights will increase the condition number of $|Dψ〉$, so the regularisation scheme will avoid these, unless there are many components in the superposition and all their weights are low.  Let's investigate what happens as the number of components and the Fock truncation are varied together.

The things I've noticed so far are:
When a superposition is iteratively fitted to the exactly propagated state, adding a term to the cost proportional to the Jacobian $|Dψ〉$ keeps the fit stable for an entire cycle.  It isn't accurate, though.

Measuring state differences in a truncated fock space makes the fit stable and accurate.  Of course, the truncated state is different from the original one, so the original state is not fit precisely.  When this was done, the least squares problem involved 6 fock coefficients, and the amplitude vector has 10 elements.  So almost all the truncated problems were underdetermined: some singular values of the Jacobian are exactly zero.  Matlab claims that backslash solves underdetermined problems by giving a solution with as many zero elements as possible.  The documentation doesn't say how it chooses which elements to make zero.  That's different from the other regularisation methods we've tried.  Of course, the problems are being solved inside Matlab's optimisation routine, and I don't know what it's doing.

%% The complete algorithm

The final algorithm would propagate $|ψ〉$ in Hilbert space by a semi-implicit method, solving a variational problem to represent each state.  This is a slight generalisation, because you need to project $H|ψ_{n+½}〉$ in to the span of $|Dψ_n〉$, and the ensembles at the two times are slightly different.  It might help to propagate from $n$ to $n+½$ by changing only weights, leaving the same ensemble.  On the other hand, the change is only that the $α*$ become $β*$, so it should be managable.

There shouldn't be an need for fancy methods to fit the initial state: one can take a guess $z₀$, and variationally solve the equation $|Dψ(z)〉dz=|ψ₀〉-|ψ(z)〉$.  One could even change the conditioning rules as the state propagates, so that $|Dψ〉$ is aligned with $|ψ₀〉-|ψ(z)〉$ to start with, then with $H|ψ(z)〉$ later on.  That way, you start the actual simulation with a well conditioned approximant.

%% Numerical experiments

I want to measure the numerical stability of the variational dynamics derived from Schrödinger's equation.  I can do that by measuring how the discretisation error converges as the timestep is reduced.  The second hypothesis is that the instability is due to stiffness, which can be tested by discretising time in ways with a variety of stability regions, and seeing whether the stability differs.


%%%%

When the least squares problem is solved with a different distance function, which sets distances along the large $N$ directions to zero, the fit fails on the first step.  Note that this isn't really a distance.  Also, I did it by solving $P|h〉=P|Dψ〉dz$; when $P$ truncates the Fock space, the operator $P|Dψ〉$ is singular.  Doing a weighted least squares fit, with the many particle directions having low weights, would be near-singular.


\bye