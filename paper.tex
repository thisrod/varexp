\input a4
\input xpmath
\input respnotes

\title{Stability of variational quantum dynamics with Gaussian approximants}

When physicists began to study quantum mechanics, they could hardly have avoided discovering coherent states.  When they grasped that observables were inherently uncertain and considered laws of probability, the normal distribution sprung to mind at once.  Once they understood the dynamics of the free particle, and looked for a more interesting system to study, the obvious choice was the harmonic oscillator.  The first question was its ground state; the second was why, in the classical limit, it oscillated.  Coherent states were the answer to both.

Variational mechanics was another obvious idea.  The quantum equations of motion were hard to solve exactly, except for some very simple systems.  It was known from very early on that the solutions were extema of certain properties: ground states had the lowest possible energy, trajectories the least possible action.  So it was natural to consider a class of possible solutions that could be treated exactly, and find out which of them had the extreme value over the class.

In the 1970s, Heller put the two ideas together, and proposed 

Coherent states have been considered as an approximant for variational quantum mechanics since 197?.  Superpositions of them are an obvious way to extend the approximant, so that the variational dynamics converge to Hamiltonian evolution.  However, these dynamics are numerically unstable, and it has not been possible to increase the size of the superposition to see this convergence.  The coherent states are overcomplete, and it has often been suggested that this instability is due to the variational problem being underdetermined.

A numerical experiment was carried out to test this hypothesis.  The dynamics of a quartic oscillator were computed by three different methods.  First, a harmonic potential was fit to the bottom of the Morse potential, the state and the Hamiltonian were expanded over its eigenstates, and Schrödinger's equation was integrated directly in Hilbert space.  Secondly, differential equations were derived for the variational parameters in the standard way, using the normal equations of the regularised variational problem, and these were solved in parameter space.  The third method was intermediate: Schrödinger's equation was solved in Hilbert space, but the state was approximated by a superposition of coherent states, updated by least squares at each timestep.  Finally, the eigenstates of this potential are known exactly: the state was expanded over them, and the exact dynamics calculated for comparison.

In a sense, expanding over Fock states is cheating.  If it were practical to do this, there would be no point using a variational method: these would be employed in many-particle problems, where an orthonormal basis such as the Fock states would be unmanagably large.  In principle, the operations required for the second and third methods could be performed on the superpositions of coherent states, using the normal equations, without projection on an orthonormal basis; in a many-particle problem, they would have to be.  However, this would raise the question whether those least-square solutions from the normal equations were stable.  By using an orthonormal basis, we can be more confident of this.

The variational methods are based on the phase space mapping $G$ from square-integrable functions to Fock space, where
$$Gf=∫f(α*)|α〉\,d²α.$$
This is different from Glauber's use: my $f$ is his $fe^{-½|α|²}$.  (Check the half.)

Hypothesis: The singular values of $G$ are 0 and $\sqrt π$, and the space belonging to $\sqrt π$ comprises the functions $f$ for which $fe^{½|α|²}$ is analytic.

There are two ways one could set about proving this.  Firstly, one could show that any function orthogonal to the Gaussian analytics annihilates $G$.  Otherwise, one could show that the norms of all Gaussian analytics are scaled by the same factor going to F
ock space, and that they are a pre-image of Fock space.  The second approach sounds simpler.

This won't be a rigourous argument: it could turn out that there's another class of expansions with half the norm, and the entire functions have components belonging to the 0 singular value.  My numerical experiment suggests otherwise; let's ignore that possibility for now.

Let $g$ be a normalised entire function, in Glauber's sense.  Our expansion function is 
$$f(α) = {1\over π}g(α)e^{-½|α|²}.$$
If $g$ has the Taylor expansion
$$g(z)=∑_n c_n{zⁿ\over(n!)^½},$$
then 
$$Gf = ∑_n c_n|n〉.$$
The functions $zⁿ$ for $n≥0$ span the entire functions (or their span is a dense subset, or something like that), so let's find the norm of 
$$f(α)={1\over π}e^{-½|α|²}{αⁿ\over(n!)^½}.$$
We have 
$$|f(α)|²={1\over π²n!}e^{-|α|²}|α|^{2n},$$
so the squared norm is
$$\|f\|²={1\over π²n!}∫₀^∞2πr^{2n+1}e^{-r²}\,dr={2\over πn!}∫₀^∞r^{2n+1}e^{-r²}\,dr.$$
According to Wolfram, this is 
$$\|f\|²={1\over π},$$
whence
$$\|f\|=π^{-½}.$$

Note that the gaussian entire functions are a vector subspace of the square integrable functions.

Assuming that there is no way to expand a state with a smaller norm than a Gaussian entire function, this does it.  Or at least I think so.  I should look up exactly what the uniqueness properties are for singular value expansions in infinite dimensional spaces.


Expanding the quartic oscillator over Fock states allows it to be solved very stably, and as precisely as desired by rotating the phases of the expansion coefficients.  Also, it is easy to find the expansion over Fock states of a superpositon of coherent states.  This means that the stability of time discretisation can be removed from the problem entirely.  The state can be propagated exactly, and the coherent states adjusted to fit.  The most direct way to do so is by nonlinear optimisation at every timestep; Matlab provides the necessary routines in a convenient package.  Unfortunately, this is not at all stable.  

\centerline{\XeTeXpicfile flwc0n10.pdf width \hsize}

\centerline{\XeTeXpicfile flwc0n5.pdf width \hsize}

The above plots show my first attempt at this.  The initial state was coherent, with real amplitude 2.  A superposition of 5 coherent states was initialised with amplitudes normally distributed around 2, the norm of the superposition being 1, and the components having equal norms.  This superposition was fit to the coherent state by minimising their distance in Hilbert space; for stability, the weights were constrained so that no component had a norm less than 0.1.  This was done by the Matlab routine {\tt fmincon}.  The initial state was truncated to Fock states $|0〉$ to $|10〉$, and propagated exactly in a Hamiltonian $\hbar a^{2\dagger}a²$ with timestep 0.03.  After each step, the fitting of the superposition to the exact state was repeated.

The results are shown in the first Figure.  The left hand plot shows the complex amplitude of the exact solution in black, and the fitted solution in blue.  The right hand plot shows the residual of the fit as a solid line, and the condition number of $|Dψ(z)〉$ at the fitted $z$ as a dotted line.  The program stopped after a short time, when the fitted amplitudes caused overflow when expanded over Fock coefficients.  

The second Figure shows the same simulation, with the states expanded over Fock states $|0〉$ to $|5〉$.  This has a significant effect on the truncated exact solution, adding an extra loop to the amplitude.  The fitting works a bit longer, but still fails at a very short time.

\centerline{\XeTeXpicfile flwc2n10.pdf width \hsize}

\centerline{\XeTeXpicfile flwc2n5.pdf width \hsize}

The next two plots show a different fitting strategy.  Instead of constraining the weights, a cost term was added in quadrature to the Hilbert space distance, proportional to the logarithm of the condition number of $|Dψ(z)〉$ for the fitted $z$.  The constant of proportionality was $10^{-5}$; this was adjusted until a good fit was obtained.  The most apparent thing is that the fit is much more stable: this is being done with timestep 0.1 instead of 0.3, but the fit stays stable for the whole period.  (I had to redraw the initial amplitudes a few times before that happened.)  It isn't a very good fit through the middle of the period: the residual increases to order 1.  The really interesting thing is what happened when, by accident, I truncated the Fock basis at $N=5$.  As before, this perturbs the trajectory in Hilbert space.  However, the perturbed trajectory can be fit very well: the residual is between $10^{-2}$ and $10^{-5}$, and this is done with quite well conditioned superpositions.

So even without stiffness in a discrete time propagation, there is something in the high energy components of $H|ψ〉$ that makes fitting the state with coherent states unstable.  This isn't too surprising: these high energy components are rapidly oscillating, and will by magnified when the inverse problem is solved with fairly smooth, low amplitude coherent states.  To fit the oscillating components, we need coherent states with large amplitudes, and low weights.  But the low weights will increase the condition number of $|Dψ〉$, so the regularisation scheme will avoid these, unless there are many components in the superposition and all their weights are low.  Let's investigate what happens as the number of components and the Fock truncation are varied together.

The things I've noticed so far are:
\item{1.} When a superposition is iteratively fitted to the exactly propagated state, adding a term to the cost proportional to the Jacobian $|Dψ〉$ keeps the fit stable for an entire cycle.  It isn't accurate, though.

\item{2.} Measuring state differences in a truncated fock space makes the fit stable and accurate.  Of course, the truncated state is different from the original one, so the original state is not fit precisely.  When this was done, the least squares problem involved 6 fock coefficients, and the amplitude vector has 10 elements.  So almost all the truncated problems were underdetermined: some singular values of the Jacobian are exactly zero.  Matlab claims that backslash solves underdetermined problems by giving a solution with as many zero elements as possible.  The documentation doesn't say how it chooses which elements to make zero.  That's different from the other regularisation methods we've tried.  Of course, the problems are being solved inside Matlab's optimisation routine, and I don't know what it's doing.

Reducing the dimension of Fock space so that the problem is underdetermined means that, when the problem is solved by SVD, we only take components along $N+1$ directions in Fock space, and map them to $N+1$ directions in parameter space.  The remaining $2R-N-1$ components of the solution are zero.  As we add extra dimensions to Fock space, we are taking components in more Fock space directions, which include more oscillating components of the wavefunction.  This is like sampling a function on a finer grid.  With these directions, we can form some of the smaller singular vectors of the full problem.

Truncating Fock space has a similar effect to truncating the singular values, assuming the singular directions belonging to small values have large particle numbers.  (And they do: when I plotted their overlaps with coherent states, they formed rings without support at the origin.)  And indeed, the trajectories for the average amplitude change in similar ways due to truncation: they pick up an extra loop in the complex plane.

As the Fock space expands with $N»2R$, there are still only $2R$ singular directions.  Presumably these converge to limits as $N$ increases.  There might be a norm on the matrices $〈N|Dψ〉$, where the truncated elements replaced by zero, that measures this.

\item{3.} The difference between the truncated state and the original one fails to satisfy the Picard condition when with respect to $|Dψ〉$.  What about $H|ψ〉$?

\item{4.} When the least squares problem is solved with a different distance function, which sets distances along the large $N$ directions to zero, the fit fails on the first step.  Note that this isn't really a distance.  Also, I did it by solving $P|h〉=P|Dψ〉dz$; when $P$ truncates the Fock space, the operator $P|Dψ〉$ is singular.  Doing a weighted least squares fit, with the many particle directions having low weights, would be near-singular.


I used three discretisation formulae for ODE, with different regions of stability.  As a first step, the Hamiltonian and a coherent state with real amplitude 2 was expanded over Fock states $|0〉$ through $|10〉$, and the resulting equations for the components solved directly.  The Hamiltonian is
$$H=\hbar a^{2\dagger}a²,$$
so the Fock states are eigenstates, although the spacing of their energies is uneven.  The quantum state at time $t$ was expanded over a truncated Fock basis $|N〉=\pmatrix{|1\rangle & |2\rangle & ⋯ & |N\rangle}$, with a column vector of coefficients $c(t)$, as
$$|ν(c(t))〉=|N〉c(t).$$
Schrödinger's equation expands to
$$|N〉Dc(t)=|D(ν\circ c)(t)〉={H\over i\hbar}|ν(c(t))〉=|N〉〈N|{H\over i\hbar}|N〉c(t),$$
whence
$$Dc(t)=-i〈N|{H\over \hbar}|N〉c(t).$$
Usually, these expansions would be approximate due to basis truncation.  In this case, the Fock states diagonalise $H$, so they are exact.  However, in general the initial state can only be approximately expanded over $|N\rangle$.  Time was discretised with the explicit Euler formula, which replaces the differential equation
$$Du(t)=f(u(t))$$
with a difference equation
$$v^{n+1}=vⁿ+τfⁿ,$$
where $tⁿ=nτ$, $vⁿ$ is meant to approximate $u(tⁿ)$, and $fⁿ=f(vⁿ)$.  Let
$$vⁿ=cⁿ\qquad{\rm and}\qquad fⁿ=-i〈N|{H\over\hbar}|N〉c(t).$$
Since $〈N|H|N〉$ is diagonal, the matrix product can be performed sparely as a dot product.  The results are shown below.  The Fock states are eigenstates of the Hamiltonian, so their amplitudes should be constant: in the exact solution of Schrödinger's equation, all the ratios plotted are exactly 1.

%\centerline{\XeTeXpicfile dry.pdf width \hsize}

These results are the textbook signs of stiffness.  As the time step is reduced from values on the order of 1, the discretised solutions diverge hugely, before converging at very small time steps.  In the standard theory of discrete approximations to $Du(t)=f(u(t))$, stability is determined by the eigenvalues of the linear transform $Df$.  In Schrödinger's equation, $f$ is the Hamiltonian, a linear transform, so in our expansion, $df=f=-i〈N|H/\hbar|N〉$, and the relevant eigenvalues are $-iEⁿ/\hbar=-in(n-1)$.  The stiffness of a differential equation is often measured by the ratio of the sizes of these eigenvalues.  For the quartic oscillator, some eigenvalues are zero, so this is not a sensible measure.  However, the largest eigenvalue, which often determines the timestep, increases quadratically with the size of the Fock basis as $N(N-1)$.

The stability region of the explicit Euler formula is the disk $|\bar k-(-1)|≤1|$.  So it is unstable for any Jacobian with complex eigenvalues, which is certainly the case for a Hamiltonian.  The implicit Euler formula, 
$$v^{n+1}=vⁿ+τf^{n+1},$$
has the opposite stability region.  For the oscillator, this becomes
$$c^{n+1}=cⁿ-iτ〈N|{H\over\hbar}|N〉c^{n+1},$$
so $c^{n+1}$ satisfies the linear equation
$$(1+iτ〈N|{H\over\hbar}|N〉)c^{n+1}=cⁿ.$$
This is still diagonal for the quartic oscillator.  To order $τ$, the implicitn formula is the same as the explicit one.  Obviously the higher powers of $τ$ make the difference when $τ$ is finite.

The results of the backward Euler formula are the opposite of the Euler formula.  Instead of increasing, the amplitudes of the larger number states decrease nearly to zero.  This distorts the trajectory of $〈a(t)〉$, as show below.

A third method is the semi-implicit one, with the formulae
$$v^{\prime n}=vⁿ+½τf^{\prime n}\qquad v^{n+1}=vⁿ+τf^{\prime n}.$$
With the Hamiltonian, this becomes
$$c^{n+1}=cⁿ-iτH\biggl(1+{iτ\over 2}H\biggr)^{-1}cⁿ.$$
If Hilbert space is considered as a real vector space, complex multiples of the eigenstates of the Hamiltonian form planes instead of lines.  The eigenstates evolve as $e^{iωt}$, so, when the dynamics are projected onto an eigenplane, the state traces out a circle, with radius equal to the absolute value of the the coefficient of the eigenstate when the initial state is expanded.  Different eigenstates circle the origin with different frequencies.

The semi-implicit formula has a special property: when the trajectory of $u(t)$ is a circle, the $vⁿ$ lie exactly on that circle, and the discretisation error is equivalent to rescaling time by a factor that depends on the frequency and the discrete timestep.  In Hilbert space, this means that the projections of the state onto eigenplanes still circle the origin exactly: only the frequencies are altered by discretisation.  So the effect of discretising quantum dynamics using the semi-implicit formula is merely to shift the energies of the eigenstates.  Of course, this can be a significant effect: it's the only difference between the harmonic and quartic oscillators.

The solutions from method 3 provide a test of the stiffness of the ODEs defined in parameter space by the least squares problem.  The step in Hilbert space is very nearly the same from one time step to the next.  If the step in parameter space changes dramatically, this would suggest that the variational solution is very sensitive to the state, and thus to some of the parameters.  This would indicate that the ODEs are stiff.

The initial state was the ground state of the Morse oscillator potential, given various momentum kicks.  The number of components in the superposition representing this was varied.  The initial superposition was found by the AMPL optimisation program, with the constraint that no component would have a weight less that ?.

I want to measure the numerical stability of the variational dynamics derived from Schrödinger's equation.  I can do that by measuring how the discretisation error converges as the timestep is reduced.  The second hypothesis is that the instability is due to stiffness, which can be tested by discretising time in ways with a variety of stability regions, and seeing whether the stability differs.

In the second choice, the state is approximated as a superposition of coherent states,
$$|ψ(z(t))〉=∑_{φ,α∈z(t)}e^{φ+αa†}|0〉=∑_{φ,α∈z(t)}e^{φ+½|α|²}|α〉.$$
Time is discretised immediately, setting
$$vⁿ=|ψ(zⁿ)〉\qquad{\rm and}\qquad fⁿ={H\over i\hbar}|ψ(zⁿ)〉,$$
so that the midpoint formula becomes
$$|ψ(z^{n+1})〉≈|ψ(z^{n-1})〉+{2τH\over i\hbar}|ψ(zⁿ)〉.$$
The approximation is due to basis truncation, as always in a least squares sense.  Writing the unknown $|ψ(z^{n+1})〉$ in terms of $|Dψ(zⁿ)〉$ and expanding over a Fock basis gives
$$ 〈N|Dψ(zⁿ)〉\bigl(z^{n+1}-zⁿ\bigr)≈〈N|ψ(z^{n-1})〉-〈N|ψ(zⁿ)〉-2iτ〈N|H|ψ(zⁿ)〉.$$
This matrix equation can be solved for $z^{n+1}-zⁿ$, which can be used to update $z$.  

The third choice, the conventional variational one, is to discretise the state first, setting
$$vⁿ=zⁿ\qquad{\rm and}\qquad |Dψ(zⁿ)〉fⁿ≈{H\over i\hbar}|ψ(zⁿ)〉.$$
The midpoint formula is used exactly, and $fⁿ$ satisfies the least squares problem
$$〈N|Dψ(zⁿ)〉fⁿ≈-i〈N|Dψ(zⁿ)〉{H\over \hbar}|ψ(zⁿ)〉.$$
Usually, this would be expanded over $〈Dψ(zⁿ)|$ to give a set of normal equations.  However, when $|Dψ(zⁿ)〉$ is near singular, the numerical stability of the normal equations is suspect.  Expanding over an orthonormal basis removes this confounding instability.

To do this, we need some bracket matrices.  We have
$$|Dψ(φ,α)〉=(1,a†)|ψ(φ,α)〉.$$
The brackets with the number states are
$$〈m|H/\hbar|n〉=n(n-1)δ_{mn}$$
and
$$〈n|Dψ(φ,α)〉=e^φ\pmatrix{{α^n\over \sqrt{n!}} &  α^{n-1}\sqrt{n\over (n-1)!}},$$
as is the bracket of the quartic oscillator Hamiltonian $a^{2\dagger}a²$
$$〈n|H/\hbar|ψ(φ,α)〉=e^φα^n\sqrt{n(n-1)\over(n-2)!}.$$

The instability in the dynamics is caused by the near-singularity of $|Dψ〉$.  If a singular value decomposition $|Dψ〉=|U〉SV†$ is taken, the singularity is indicated by small singular values on the diagonal of $S$.  It is often measured by the condition number of $|Dψ〉$, the ratio of the largest to the smallest singular value.  Let $C(z)$ be the condition number of $|Dψ(z)〉$.  As part of the regularisation process, it would be useful to avoid changing $z$ in directions that increase $C(z)$.  For example, if two coherent states had real amplitudes, and the real part of the expected amplitude needed to increase, it would be done by shifting the right hand one away from the left hand one, not the left hand one closer to the right hand.  Is there a way to make a computer do that?

The least squares problem can be solved using the singular value decomposition: to approximate a state $|h〉≈|Dψ(z)〉dz$, we set $dz=VS^{-1}〈U|h〉$.  The first problem is that some of the $σ_i$ on the diagonal of $S$ will be small, because the $|u_i〉$ that correspond to them change different components of the superposition $|ψ(z)〉$ in ways that nearly cancel out.  So a error in the discrete representation of $|h〉$ in the direction $|u_i〉$ will cause a very large error in the least squares $dz$ in the direction $v_i$.   The way to avoid such large errors is to shift the small singular values of $S$ up to a lower bound $ε$, and thus reduce the large singular values of $S^{-1}$ below an upper bound $ε^{-1}$.  There are many and varied ways of doing that. 

To solving our differential equation, we need to solve an iterated least squares problem.  We need to do so in a way that preserves the condition of $|Dψ〉$.  When we shift $z$ in the direction $v_i$ as $dz=λv_i$, how does this condition number change?  Without loss of generality, it suffices to consider the effect on the smallest and largest singular values, that determine $C(z)$.  Note that $v_1$ and $v_R$ are eigenvectors of the matrix ${\cal V}(z)=〈Dψ(z)|Dψ(z)〉$, belonging to the eigenvalues $σ₁²$ and $σ_R²$; this is a familiar problem in pertubation theory.  Assume for now that we have found $D{\cal V}(z)$—this won't be analytic, so we will need to use real and imaginary parts, or Wirtinger calculus.  The change in $\cal V$ is of the form $d{\cal V}=∂_i{\cal V}(z)λ+∂*_i{\cal V}(z)λ*$, where the partial derivatives are taken in the directions $v_i$.

As is well known in pertubation theory, the change in the eigenvalue $σ₁²$ corresponding to a small change $d{\cal V}$ is $v₁†d{\cal V}v₁$, and similarly for $σ_R²$.  Therefore the change in $C(z)²$ is 
$$σ_R²v₁†d{\cal V}v₁-σ₁²v_R†d{\cal V}v_R \over σ_R².$$
Given a proposed change $dz$, we can find the effect on the condition number.

The question is how to apply this to condition the process of finding $dz$.  Given $z$, we can find out which directions of $dz$ correspond to large changes in the condition number.  However,  until we know $dz$, we don't know if these changes help or hurt.  The process might run like this: first, truncate the singular values.  Next, find the components $〈U_i|h〉$, and the corresponding components $v_iσ_i〈U_i|h〉$ of $dz$, given the truncation.  Then, truncate the components that will cause $C(z)$ to increase to values that limits the increase to an acceptable amount.

Coding this will be quite involved, and the result might not be very efficient.  However, if it allows physically chosen bases to be used in  variational simulations, it is likely to be worthwhile.

\centerline{\XeTeXpicfile j1.pdf width \hsize}

To test this idea, a five component superposition was fit to an amplitude 2 coherent state.  The Hilbert space residual was $1.3×10^{-3}$.  The amplitudes and norms of the components are shown above; the fitting routine constrained the norm of each component to be at least 0.1.  Note that components 1 and 3 ended up with very similar amplitudes.

Below is a plot of the condition number of $|Dψ(z)〉$ for this superposition, as the value of $z$ varies by $0.1$ in every direction.  The top line shows the effects of changing $φ$, the bottom line $α$, components are shown left to right.  Changes to real parts are black, imaginary parts red.  There are a few things to note.  The condition is very bad, with number $1.4×10^{11}$.  This is entirely due to the similar amplitudes of components 1 and 3: those graphs are on a different scale to the others, and it's logarithmic.  As they move closer and become identical, the condition number increases $10^{14}$.  No doubt it would go higher if we sampled the graphs more finely, but this is huge—reciprocal floating point epsilon is $5×10^{15}$.  If the separation between these components is doubled, the condition number improves by an order of magnitude.

\centerline{\XeTeXpicfile j2.pdf width \hsize}

The top line indicates that the condition number is independent of the phases of the components.  I didn't forsee that, but it isn't too surprising: we can change the coefficients of the superposition in any direction we like, regardless of their current values, and the derivative is only sensitive to changes.

The experiment with following the exact solution by nonlinear optimisation was repeated.  Instead of constraining the weights of the components, the logarithm of the condition number of the derivative was added in quadrature to the residual of the fit to give a cost.  This worked much better, getting stably to $t=0.4$ instead of under 0.1.  At that point, the residuals increased to order 1, and the superposition stopped following the exact solution.  This cold mean that 5 components weren't enough to represent those states, except that the same thing happened with 10 components.

Note that the optimisation is a nonlinear least squares problem, and special solvers exist for this case.

Some furt


\bye